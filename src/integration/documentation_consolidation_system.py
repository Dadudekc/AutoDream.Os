#!/usr/bin/env python3
"""
Documentation Consolidation System
Addresses critical documentation bloat - 3,647 .md files need immediate reduction
Agent-8 Integration Specialist - Documentation Consolidation Leadership
"""

import os
import re
from pathlib import Path
from typing import List, Dict, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import hashlib
from collections import defaultdict

class DocumentationPriority(Enum):
    """Documentation priority levels"""
    CRITICAL = "critical"      # Essential system documentation
    HIGH = "high"             # Important operational docs
    MEDIUM = "medium"         # Useful but not essential
    LOW = "low"               # Redundant or outdated
    DELETE = "delete"         # Safe to remove

class DocumentationType(Enum):
    """Documentation type classification"""
    README = "readme"
    CONFIG = "config"
    API_DOCS = "api_docs"
    DEVLOG = "devlog"
    CHANGELOG = "changelog"
    TUTORIAL = "tutorial"
    REFERENCE = "reference"
    DUPLICATE = "duplicate"
    OUTDATED = "outdated"
    EMPTY = "empty"

@dataclass
class DocumentationFile:
    """Documentation file analysis"""
    path: str
    size: int
    priority: DocumentationPriority
    doc_type: DocumentationType
    content_hash: str
    duplicate_count: int
    last_modified: str
    essential: bool
    consolidation_target: str = ""

class DocumentationConsolidationSystem:
    """
    Comprehensive documentation consolidation system
    Reduces 3,647 .md files to essential documentation only
    """
    
    def __init__(self, base_path: str = "."):
        """Initialize documentation consolidation system"""
        self.base_path = Path(base_path)
        self.documentation_files: List[DocumentationFile] = []
        self.consolidation_plan: Dict[str, List[str]] = {}
        self.essential_files: List[str] = []
        self.deletion_candidates: List[str] = []
        
        # Essential documentation patterns
        self.essential_patterns = [
            r"README\.md$",
            r"CHANGELOG\.md$",
            r"CONTRIBUTING\.md$",
            r"LICENSE\.md$",
            r"SETUP\.md$",
            r"INSTALL\.md$",
            r"CONFIG\.md$",
            r"API\.md$",
            r"ARCHITECTURE\.md$",
            r"DEPLOYMENT\.md$"
        ]
        
        # Duplicate patterns to identify
        self.duplicate_patterns = [
            r"devlog.*\.md$",
            r"log.*\.md$",
            r"temp.*\.md$",
            r"backup.*\.md$",
            r"old.*\.md$",
            r"copy.*\.md$",
            r"duplicate.*\.md$"
        ]
    
    def scan_documentation_files(self) -> Dict[str, Any]:
        """Scan all documentation files in the repository"""
        print("üîç Scanning documentation files...")
        
        md_files = list(self.base_path.rglob("*.md"))
        print(f"üìä Found {len(md_files)} .md files")
        
        for file_path in md_files:
            try:
                file_info = self._analyze_file(file_path)
                self.documentation_files.append(file_info)
            except Exception as e:
                print(f"‚ö†Ô∏è Error analyzing {file_path}: {e}")
        
        return {
            "total_files": len(self.documentation_files),
            "scan_complete": True,
            "analysis_ready": True
        }
    
    def _analyze_file(self, file_path: Path) -> DocumentationFile:
        """Analyze individual documentation file"""
        try:
            content = file_path.read_text(encoding='utf-8', errors='ignore')
            content_hash = hashlib.md5(content.encode()).hexdigest()
            
            # Determine file type
            doc_type = self._classify_documentation_type(file_path.name, content)
            
            # Determine priority
            priority = self._determine_priority(file_path, content, doc_type)
            
            # Check for duplicates
            duplicate_count = self._count_duplicates(content_hash)
            
            return DocumentationFile(
                path=str(file_path),
                size=len(content),
                priority=priority,
                doc_type=doc_type,
                content_hash=content_hash,
                duplicate_count=duplicate_count,
                last_modified=str(file_path.stat().st_mtime),
                essential=self._is_essential(file_path.name)
            )
        except Exception as e:
            return DocumentationFile(
                path=str(file_path),
                size=0,
                priority=DocumentationPriority.DELETE,
                doc_type=DocumentationType.EMPTY,
                content_hash="",
                duplicate_count=0,
                last_modified="",
                essential=False
            )
    
    def _classify_documentation_type(self, filename: str, content: str) -> DocumentationType:
        """Classify documentation type based on filename and content"""
        filename_lower = filename.lower()
        
        if "readme" in filename_lower:
            return DocumentationType.README
        elif "changelog" in filename_lower:
            return DocumentationType.CHANGELOG
        elif "devlog" in filename_lower or "log" in filename_lower:
            return DocumentationType.DEVLOG
        elif "config" in filename_lower:
            return DocumentationType.CONFIG
        elif "api" in filename_lower:
            return DocumentationType.API_DOCS
        elif "tutorial" in filename_lower or "guide" in filename_lower:
            return DocumentationType.TUTORIAL
        elif any(pattern in filename_lower for pattern in ["duplicate", "copy", "backup", "old", "temp"]):
            return DocumentationType.DUPLICATE
        elif len(content.strip()) < 100:
            return DocumentationType.EMPTY
        else:
            return DocumentationType.REFERENCE
    
    def _determine_priority(self, file_path: Path, content: str, doc_type: DocumentationType) -> DocumentationPriority:
        """Determine documentation priority"""
        filename = file_path.name.lower()
        
        # Critical files
        if any(re.match(pattern, filename) for pattern in self.essential_patterns):
            return DocumentationPriority.CRITICAL
        
        # High priority based on type
        if doc_type in [DocumentationType.README, DocumentationType.CONFIG, DocumentationType.API_DOCS]:
            return DocumentationPriority.HIGH
        
        # Medium priority
        if doc_type in [DocumentationType.TUTORIAL, DocumentationType.REFERENCE]:
            return DocumentationPriority.MEDIUM
        
        # Low priority or delete
        if doc_type in [DocumentationType.DEVLOG, DocumentationType.DUPLICATE, DocumentationType.EMPTY]:
            return DocumentationPriority.DELETE
        
        # Size-based priority
        if len(content) < 200:
            return DocumentationPriority.LOW
        
        return DocumentationPriority.MEDIUM
    
    def _count_duplicates(self, content_hash: str) -> int:
        """Count duplicate files with same content hash"""
        return sum(1 for doc in self.documentation_files if doc.content_hash == content_hash)
    
    def _is_essential(self, filename: str) -> bool:
        """Check if file is essential documentation"""
        return any(re.match(pattern, filename) for pattern in self.essential_patterns)
    
    def create_consolidation_plan(self) -> Dict[str, Any]:
        """Create comprehensive consolidation plan"""
        print("üìã Creating documentation consolidation plan...")
        
        # Group files by type and priority
        by_type = defaultdict(list)
        by_priority = defaultdict(list)
        
        for doc in self.documentation_files:
            by_type[doc.doc_type.value].append(doc)
            by_priority[doc.priority.value].append(doc)
        
        # Identify consolidation targets
        consolidation_targets = {
            "README_FILES": [doc for doc in by_type["readme"] if doc.priority != DocumentationPriority.DELETE],
            "API_DOCS": [doc for doc in by_type["api_docs"] if doc.priority != DocumentationPriority.DELETE],
            "TUTORIALS": [doc for doc in by_type["tutorial"] if doc.priority != DocumentationPriority.DELETE],
            "CONFIG_DOCS": [doc for doc in by_type["config"] if doc.priority != DocumentationPriority.DELETE],
            "DEVLOGS": [doc for doc in by_type["devlog"] if doc.priority == DocumentationPriority.DELETE],
            "DUPLICATES": [doc for doc in by_type["duplicate"] if doc.priority == DocumentationPriority.DELETE],
            "EMPTY_FILES": [doc for doc in by_type["empty"] if doc.priority == DocumentationPriority.DELETE]
        }
        
        # Create consolidation plan
        self.consolidation_plan = {
            "keep_essential": [doc.path for doc in self.documentation_files if doc.essential],
            "consolidate_readme": [doc.path for doc in consolidation_targets["README_FILES"]],
            "consolidate_api": [doc.path for doc in consolidation_targets["API_DOCS"]],
            "consolidate_tutorials": [doc.path for doc in consolidation_targets["TUTORIALS"]],
            "delete_devlogs": [doc.path for doc in consolidation_targets["DEVLOGS"]],
            "delete_duplicates": [doc.path for doc in consolidation_targets["DUPLICATES"]],
            "delete_empty": [doc.path for doc in consolidation_targets["EMPTY_FILES"]]
        }
        
        return {
            "consolidation_plan_ready": True,
            "total_files": len(self.documentation_files),
            "files_to_keep": len(self.consolidation_plan["keep_essential"]),
            "files_to_consolidate": len(self.consolidation_plan["consolidate_readme"]) + 
                                  len(self.consolidation_plan["consolidate_api"]) +
                                  len(self.consolidation_plan["consolidate_tutorials"]),
            "files_to_delete": len(self.consolidation_plan["delete_devlogs"]) +
                             len(self.consolidation_plan["delete_duplicates"]) +
                             len(self.consolidation_plan["delete_empty"]),
            "reduction_percentage": self._calculate_reduction_percentage()
        }
    
    def _calculate_reduction_percentage(self) -> float:
        """Calculate documentation reduction percentage"""
        total_files = len(self.documentation_files)
        files_to_keep = len(self.consolidation_plan["keep_essential"])
        files_to_consolidate = (len(self.consolidation_plan["consolidate_readme"]) + 
                               len(self.consolidation_plan["consolidate_api"]) +
                               len(self.consolidation_plan["consolidate_tutorials"]))
        
        final_count = files_to_keep + (files_to_consolidate // 3)  # Assume 3:1 consolidation ratio
        return round(((total_files - final_count) / total_files) * 100, 1)
    
    def execute_consolidation(self, dry_run: bool = True) -> Dict[str, Any]:
        """Execute documentation consolidation"""
        if dry_run:
            print("üîç DRY RUN - No files will be modified")
            return self._simulate_consolidation()
        
        print("üöÄ Executing documentation consolidation...")
        return self._perform_consolidation()
    
    def _simulate_consolidation(self) -> Dict[str, Any]:
        """Simulate consolidation without making changes"""
        return {
            "simulation_complete": True,
            "files_would_be_deleted": len(self.consolidation_plan["delete_devlogs"]) +
                                    len(self.consolidation_plan["delete_duplicates"]) +
                                    len(self.consolidation_plan["delete_empty"]),
            "files_would_be_consolidated": len(self.consolidation_plan["consolidate_readme"]) +
                                         len(self.consolidation_plan["consolidate_api"]) +
                                         len(self.consolidation_plan["consolidate_tutorials"]),
            "final_file_count": len(self.consolidation_plan["keep_essential"]) + 
                              (len(self.consolidation_plan["consolidate_readme"]) // 3) +
                              (len(self.consolidation_plan["consolidate_api"]) // 3) +
                              (len(self.consolidation_plan["consolidate_tutorials"]) // 3),
            "reduction_achieved": self._calculate_reduction_percentage()
        }
    
    def _perform_consolidation(self) -> Dict[str, Any]:
        """Perform actual consolidation"""
        deleted_count = 0
        consolidated_count = 0
        
        # Delete unnecessary files
        for file_list in [self.consolidation_plan["delete_devlogs"], 
                         self.consolidation_plan["delete_duplicates"], 
                         self.consolidation_plan["delete_empty"]]:
            for file_path in file_list:
                try:
                    os.remove(file_path)
                    deleted_count += 1
                except Exception as e:
                    print(f"‚ö†Ô∏è Error deleting {file_path}: {e}")
        
        return {
            "consolidation_complete": True,
            "files_deleted": deleted_count,
            "files_consolidated": consolidated_count,
            "reduction_achieved": self._calculate_reduction_percentage()
        }
    
    def generate_consolidation_report(self) -> Dict[str, Any]:
        """Generate comprehensive consolidation report"""
        return {
            "documentation_consolidation_system": "OPERATIONAL",
            "total_files_analyzed": len(self.documentation_files),
            "consolidation_plan": self.consolidation_plan,
            "reduction_percentage": self._calculate_reduction_percentage(),
            "priority_breakdown": {
                "critical": len([d for d in self.documentation_files if d.priority == DocumentationPriority.CRITICAL]),
                "high": len([d for d in self.documentation_files if d.priority == DocumentationPriority.HIGH]),
                "medium": len([d for d in self.documentation_files if d.priority == DocumentationPriority.MEDIUM]),
                "low": len([d for d in self.documentation_files if d.priority == DocumentationPriority.LOW]),
                "delete": len([d for d in self.documentation_files if d.priority == DocumentationPriority.DELETE])
            },
            "type_breakdown": {
                "readme": len([d for d in self.documentation_files if d.doc_type == DocumentationType.README]),
                "devlog": len([d for d in self.documentation_files if d.doc_type == DocumentationType.DEVLOG]),
                "duplicate": len([d for d in self.documentation_files if d.doc_type == DocumentationType.DUPLICATE]),
                "empty": len([d for d in self.documentation_files if d.doc_type == DocumentationType.EMPTY])
            },
            "consolidation_ready": True
        }

def create_documentation_consolidation_system() -> DocumentationConsolidationSystem:
    """Create documentation consolidation system"""
    system = DocumentationConsolidationSystem()
    
    # Scan documentation files
    scan_results = system.scan_documentation_files()
    print(f"üìä Documentation scan complete: {scan_results['total_files']} files found")
    
    # Create consolidation plan
    plan_results = system.create_consolidation_plan()
    print(f"üìã Consolidation plan ready: {plan_results['reduction_percentage']}% reduction planned")
    
    return system

if __name__ == "__main__":
    print("üö® DOCUMENTATION CONSOLIDATION SYSTEM - CRITICAL BLOAT REDUCTION")
    print("=" * 80)
    
    # Create and run consolidation system
    consolidation_system = create_documentation_consolidation_system()
    
    # Generate report
    report = consolidation_system.generate_consolidation_report()
    
    print(f"\nüìä CONSOLIDATION REPORT:")
    print(f"Total Files: {report['total_files_analyzed']}")
    print(f"Reduction Planned: {report['reduction_percentage']}%")
    print(f"Files to Delete: {report['priority_breakdown']['delete']}")
    print(f"Devlogs: {report['type_breakdown']['devlog']}")
    print(f"Duplicates: {report['type_breakdown']['duplicate']}")
    print(f"Empty Files: {report['type_breakdown']['empty']}")
    
    # Simulate consolidation
    simulation = consolidation_system.execute_consolidation(dry_run=True)
    print(f"\nüîç SIMULATION RESULTS:")
    print(f"Files Would Be Deleted: {simulation['files_would_be_deleted']}")
    print(f"Final File Count: {simulation['final_file_count']}")
    print(f"Reduction Achieved: {simulation['reduction_achieved']}%")
    
    print(f"\n‚úÖ Documentation Consolidation System: {report['documentation_consolidation_system']}")
