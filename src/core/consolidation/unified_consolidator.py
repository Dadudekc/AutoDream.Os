import os
import ast
import logging
from dataclasses import dataclass
from typing import Any, Callable, Dict, List

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)


@dataclass
class ResourceDefinition:
    """Represents a resource extracted from a file."""
    name: str
    value: Any
    file_path: str
    line_number: int
    resource_type: str = "unknown"
    description: str = ""
    category: str = "unknown"


class UnifiedConsolidator:
    """Generic consolidator for constants, metrics, and other resources."""

    def __init__(
        self,
        pattern: str,
        extractor: Callable[[str], List[ResourceDefinition]],
        categorize: Callable[[ResourceDefinition], str] | None = None,
    ) -> None:
        self.pattern = pattern
        self.extractor = extractor
        self.categorize = categorize or self._default_categorize

    def find_files(self) -> List[str]:
        """Locate files matching the consolidation pattern."""
        files: List[str] = []
        for root, _, filenames in os.walk("."):
            if self.pattern in filenames:
                files.append(os.path.join(root, self.pattern))
        logger.info("Found %s %s files", len(files), self.pattern)
        return files

    def consolidate(self, output_path: str | None = None) -> Dict[str, Any]:
        """Run consolidation and optionally write a consolidated file."""
        files = self.find_files()
        resources: List[ResourceDefinition] = []
        for file_path in files:
            resources.extend(self.extractor(file_path))

        categorized: Dict[str, List[ResourceDefinition]] = {}
        duplicates = 0
        for res in resources:
            res.category = self.categorize(res)
            bucket = categorized.setdefault(res.category, [])
            existing = next((r for r in bucket if r.name == res.name), None)
            if existing:
                duplicates += 1
                if len(res.file_path) < len(existing.file_path):
                    bucket.remove(existing)
                    bucket.append(res)
            else:
                bucket.append(res)

        if output_path:
            self._write_output(output_path, categorized)

        return {
            "pattern": self.pattern,
            "files_processed": len(files),
            "resources_extracted": len(resources),
            "duplicates_eliminated": duplicates,
            "categories": {cat: len(vals) for cat, vals in categorized.items()},
            "files_to_remove": files,
        }

    def _write_output(
        self, path: str, categorized: Dict[str, List[ResourceDefinition]]
    ) -> None:
        os.makedirs(os.path.dirname(path), exist_ok=True)
        with open(path, "w", encoding="utf-8") as f:
            f.write("# Consolidated resources generated by UnifiedConsolidator\n")
            for category, resources in categorized.items():
                f.write(f"# === {category} ===\n")
                for res in resources:
                    f.write(
                        f"# {res.file_path}:{res.line_number} - {res.name}\n"
                    )
            f.write("CONSOLIDATION_INFO = {\n")
            f.write(f"    'pattern': '{self.pattern}',\n")
            f.write(
                f"    'total_files': {sum(len(v) for v in categorized.values())},\n"
            )
            f.write(f"    'categories': {len(categorized)},\n")
            f.write("}\n")

    def _default_categorize(self, resource: ResourceDefinition) -> str:
        path = resource.file_path.lower()
        for key, category in _CATEGORY_MAP.items():
            if key in path:
                return category
        return "general"


_CATEGORY_MAP = {
    "task": "task_management",
    "scheduler": "task_management",
    "workflow": "workflow",
    "agent": "agent_management",
    "refactoring": "refactoring",
    "optimization": "optimization",
    "baseline": "baseline",
    "handoff": "handoff",
    "status": "status_monitoring",
    "ai_ml": "ai_ml",
    "health": "health_monitoring",
    "fsm": "fsm",
    "communication": "communication",
    "web": "web_frontend",
    "frontend": "web_frontend",
    "testing": "testing",
    "service": "services",
    "config": "configuration",
    "decision": "decision_making",
    "manager": "management",
}


def extract_constant_definitions(file_path: str) -> List[ResourceDefinition]:
    resources: List[ResourceDefinition] = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read())
        for node in ast.walk(tree):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name) and target.id.isupper():
                        try:
                            value = ast.literal_eval(node.value)
                        except Exception:
                            value = ast.unparse(node.value)
                        resources.append(
                            ResourceDefinition(
                                name=target.id,
                                value=value,
                                file_path=file_path,
                                line_number=node.lineno,
                                resource_type="constant",
                                description=f"Extracted from {file_path}",
                            )
                        )
    except Exception as e:
        logger.error("Error extracting constants from %s: %s", file_path, e)
    return resources


def extract_metric_definitions(file_path: str) -> List[ResourceDefinition]:
    resources: List[ResourceDefinition] = []
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            tree = ast.parse(f.read())
        for node in ast.walk(tree):
            if isinstance(node, ast.Assign):
                for target in node.targets:
                    if isinstance(target, ast.Name):
                        try:
                            value = ast.literal_eval(node.value)
                        except Exception:
                            value = ast.unparse(node.value)
                        resources.append(
                            ResourceDefinition(
                                name=target.id,
                                value=value,
                                file_path=file_path,
                                line_number=node.lineno,
                                resource_type="metric",
                                description=f"Assignment from {file_path}",
                            )
                        )
            elif isinstance(node, ast.ClassDef):
                resources.append(
                    ResourceDefinition(
                        name=node.name,
                        value=f"class {node.name}",
                        file_path=file_path,
                        line_number=node.lineno,
                        resource_type="class",
                        description=f"Class from {file_path}",
                    )
                )
            elif isinstance(node, ast.FunctionDef):
                resources.append(
                    ResourceDefinition(
                        name=node.name,
                        value=f"def {node.name}",
                        file_path=file_path,
                        line_number=node.lineno,
                        resource_type="function",
                        description=f"Function from {file_path}",
                    )
                )
    except Exception as e:
        logger.error("Error extracting metrics from %s: %s", file_path, e)
    return resources


def consolidate_constants(output_path: str | None = None) -> Dict[str, Any]:
    consolidator = UnifiedConsolidator("constants.py", extract_constant_definitions)
    return consolidator.consolidate(output_path)


def consolidate_metrics(output_path: str | None = None) -> Dict[str, Any]:
    consolidator = UnifiedConsolidator("metrics.py", extract_metric_definitions)
    return consolidator.consolidate(output_path)


__all__ = [
    "ResourceDefinition",
    "UnifiedConsolidator",
    "extract_constant_definitions",
    "extract_metric_definitions",
    "consolidate_constants",
    "consolidate_metrics",
]
