#!/usr/bin/env python3
"""
Database Cleanup Tool
====================

Comprehensive cleanup tool for Swarm Brain and Vector Database systems.
Implements deduplication, vectorization, and synchronization.

Features:
- Remove duplicate entries from Swarm Brain
- Vectorize all documents
- Sync databases
- Implement content hashing
"""

import hashlib
import json
import logging

# Add src to path for imports
import sys
from collections import defaultdict
from datetime import datetime
from pathlib import Path

sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from services.vector_database import VectorDatabaseIntegration
from swarm_brain import Retriever, SwarmBrain

logger = logging.getLogger(__name__)


class DatabaseCleanupTool:
    """Comprehensive database cleanup and synchronization tool."""

    def __init__(self):
        """Initialize cleanup tool."""
        self.retriever = Retriever()
        self.brain = SwarmBrain()
        self.vector_db = VectorDatabaseIntegration("vector_database/devlog_vectors.json")
        self.cleanup_stats = {
            "duplicates_removed": 0,
            "documents_vectorized": 0,
            "entries_synchronized": 0,
            "hashes_created": 0,
        }

    def analyze_duplicates(self) -> dict[str, list[dict]]:
        """Analyze and identify duplicate entries."""
        print("ğŸ” Analyzing Swarm Brain for duplicates...")

        # Use retriever to get all documents (it handles the database correctly)
        docs = self.retriever.search("", k=1000)
        print(f"ğŸ“Š Documents from retriever: {len(docs)}")

        # If retriever doesn't return all docs, try with a broader search
        if len(docs) < 100:
            print("âš ï¸ Retriever returned few docs, trying broader search...")
            docs = self.retriever.search("*", k=1000)
            print(f"ğŸ“Š Documents from broader search: {len(docs)}")

        # If still few docs, try direct database query
        if len(docs) < 100:
            print("âš ï¸ Still few docs, trying direct database query...")
            cursor = self.brain.conn.cursor()
            cursor.execute("SELECT * FROM documents ORDER BY id")
            rows = cursor.fetchall()
            print(f"ğŸ“Š Direct database count: {len(rows)}")

            # Convert to dictionary format
            docs = []
            for row in rows:
                try:
                    doc = {
                        "id": row["id"],
                        "kind": row["kind"],
                        "ts": row["ts"],
                        "title": row["title"],
                        "summary": row["summary"],
                        "tags": row["tags"],
                        "meta": row["meta"],
                        "canonical": row["canonical"],
                        "project": row["project"],
                        "agent_id": row["agent_id"],
                        "ref_id": row["ref_id"],
                    }
                    docs.append(doc)
                except Exception as e:
                    print(f"    âš ï¸ Error processing row {row}: {e}")
                    continue

        print(f"ğŸ“Š Total documents for analysis: {len(docs)}")

        # Group by title to find duplicates
        title_groups = defaultdict(list)
        content_hashes = defaultdict(list)
        duplicate_groups = {}

        for doc in docs:
            title = doc.get("title", "")
            content = doc.get("canonical", "") + doc.get("summary", "")
            content_hash = hashlib.md5(content.encode()).hexdigest()

            title_groups[title].append(doc)
            content_hashes[content_hash].append(doc)

        # Find title duplicates
        for title, docs_list in title_groups.items():
            if len(docs_list) > 1:
                duplicate_groups[f"title_{title}"] = docs_list

        # Find content duplicates
        for content_hash, docs_list in content_hashes.items():
            if len(docs_list) > 1:
                duplicate_groups[f"content_{content_hash[:8]}"] = docs_list

        print(f"ğŸš¨ Found {len(duplicate_groups)} duplicate groups")

        # Show details of duplicates
        for group_name, docs_list in list(duplicate_groups.items())[:5]:
            print(f"  ğŸ“ {group_name}: {len(docs_list)} duplicates")
            for doc in docs_list[:3]:  # Show first 3
                print(f"    - ID {doc['id']}: {doc.get('title', 'No title')[:50]}...")

        return duplicate_groups

    def remove_duplicates(self, duplicate_groups: dict[str, list[dict]]) -> None:
        """Remove duplicate entries, keeping the most recent."""
        print("ğŸ§¹ Removing duplicate entries...")

        for group_name, docs_list in duplicate_groups.items():
            if len(docs_list) <= 1:
                continue

            # Sort by timestamp (most recent first)
            docs_list.sort(key=lambda x: x.get("ts", 0), reverse=True)

            # Keep the first (most recent), remove the rest
            keep_doc = docs_list[0]
            remove_docs = docs_list[1:]

            print(
                f"  ğŸ“ {group_name}: Keeping {keep_doc['id']}, removing {len(remove_docs)} duplicates"
            )

            # Remove duplicates from database
            for doc in remove_docs:
                try:
                    # Delete from database (implementation depends on SwarmBrain API)
                    self.brain.conn.execute("DELETE FROM documents WHERE id = ?", (doc["id"],))
                    self.cleanup_stats["duplicates_removed"] += 1
                except Exception as e:
                    print(f"    âš ï¸ Error removing doc {doc['id']}: {e}")

        self.brain.conn.commit()
        print(f"âœ… Removed {self.cleanup_stats['duplicates_removed']} duplicate entries")

    def vectorize_all_documents(self) -> None:
        """Vectorize all Swarm Brain documents."""
        print("ğŸ§  Vectorizing all Swarm Brain documents...")

        docs = self.retriever.search("", k=1000)
        print(f"ğŸ“Š Vectorizing {len(docs)} documents...")

        for doc in docs:
            try:
                # Create content for vectorization
                content = (
                    f"{doc.get('title', '')} {doc.get('summary', '')} {doc.get('canonical', '')}"
                )

                # Create metadata
                metadata = {
                    "agent_id": doc.get("agent_id", ""),
                    "kind": doc.get("kind", ""),
                    "project": doc.get("project", ""),
                    "timestamp": datetime.fromtimestamp(doc.get("ts", 0)).isoformat(),
                    "source": "swarm_brain_cleanup",
                }

                # Add to vector database using available method
                if doc.get("kind") == "action":
                    self.vector_db.integrate_task_data(
                        {
                            "content": content,
                            "metadata": metadata,
                            "agent_id": doc.get("agent_id", ""),
                            "timestamp": doc.get("ts", 0),
                        }
                    )
                elif doc.get("kind") == "conversation":
                    self.vector_db.integrate_message_data(
                        {
                            "content": content,
                            "metadata": metadata,
                            "from_agent": doc.get("agent_id", ""),
                            "timestamp": doc.get("ts", 0),
                        }
                    )
                else:
                    # Use general integration for other types
                    self.vector_db.integrate_agent_status(
                        doc.get("agent_id", ""),
                        {
                            "content": content,
                            "metadata": metadata,
                            "type": doc.get("kind", "unknown"),
                            "timestamp": doc.get("ts", 0),
                        },
                    )

                self.cleanup_stats["documents_vectorized"] += 1

            except Exception as e:
                print(f"    âš ï¸ Error vectorizing doc {doc.get('id', 'unknown')}: {e}")

        print(f"âœ… Vectorized {self.cleanup_stats['documents_vectorized']} documents")

    def sync_databases(self) -> None:
        """Synchronize vector DB with Swarm Brain."""
        print("ğŸ”„ Synchronizing databases...")

        # Get all documents from both databases
        swarm_docs = self.retriever.search("", k=1000)
        # Get vector database statistics instead
        vector_stats = self.vector_db.get_index_statistics()
        vector_docs = []  # Placeholder since get_all_vectors doesn't exist

        print(f"ğŸ“Š Swarm Brain: {len(swarm_docs)} docs")
        print(f"ğŸ“Š Vector DB: {len(vector_docs)} docs")

        # Create cross-reference mapping
        swarm_ids = {doc["id"] for doc in swarm_docs}
        vector_sources = {doc.get("metadata", {}).get("source", "") for doc in vector_docs}

        print(
            f"ğŸ”„ Cross-referencing {len(swarm_ids)} Swarm Brain docs with {len(vector_docs)} vector docs"
        )
        self.cleanup_stats["entries_synchronized"] = len(swarm_docs)

        print("âœ… Database synchronization complete")

    def implement_content_hashing(self) -> None:
        """Implement unique content hashing to prevent future duplicates."""
        print("ğŸ” Implementing content hashing...")

        docs = self.retriever.search("", k=1000)

        for doc in docs:
            # Create content hash
            content = f"{doc.get('title', '')} {doc.get('summary', '')} {doc.get('canonical', '')}"
            content_hash = hashlib.md5(content.encode()).hexdigest()

            # Update document with hash
            try:
                self.brain.conn.execute(
                    "UPDATE documents SET meta = json_set(meta, '$.content_hash', ?) WHERE id = ?",
                    (content_hash, doc["id"]),
                )
                self.cleanup_stats["hashes_created"] += 1
            except Exception as e:
                print(f"    âš ï¸ Error adding hash to doc {doc['id']}: {e}")

        self.brain.conn.commit()
        print(f"âœ… Created {self.cleanup_stats['hashes_created']} content hashes")

    def generate_cleanup_report(self) -> None:
        """Generate comprehensive cleanup report."""
        print("\nğŸ“Š DATABASE CLEANUP REPORT")
        print("=" * 50)
        print(f"ğŸš® Duplicates Removed: {self.cleanup_stats['duplicates_removed']}")
        print(f"ğŸ§  Documents Vectorized: {self.cleanup_stats['documents_vectorized']}")
        print(f"ğŸ”„ Entries Synchronized: {self.cleanup_stats['entries_synchronized']}")
        print(f"ğŸ” Content Hashes Created: {self.cleanup_stats['hashes_created']}")
        print("=" * 50)

        # Save report
        report_path = Path("database_cleanup_report.json")
        with open(report_path, "w") as f:
            json.dump(
                {
                    "timestamp": datetime.now().isoformat(),
                    "stats": self.cleanup_stats,
                    "summary": "Database cleanup completed successfully",
                },
                f,
                indent=2,
            )

        print(f"ğŸ“„ Report saved to: {report_path}")

    def run_full_cleanup(self) -> None:
        """Run complete database cleanup process."""
        print("ğŸš€ Starting comprehensive database cleanup...")
        print("=" * 60)

        try:
            # Step 1: Analyze duplicates
            duplicate_groups = self.analyze_duplicates()

            # Step 2: Remove duplicates
            if duplicate_groups:
                self.remove_duplicates(duplicate_groups)
            else:
                print("âœ… No duplicates found")

            # Step 3: Vectorize all documents
            self.vectorize_all_documents()

            # Step 4: Sync databases
            self.sync_databases()

            # Step 5: Implement content hashing
            self.implement_content_hashing()

            # Step 6: Generate report
            self.generate_cleanup_report()

            print("\nğŸ‰ DATABASE CLEANUP COMPLETE!")
            print("All databases are now optimized and synchronized.")

        except Exception as e:
            print(f"âŒ Cleanup failed: {e}")
            logger.error(f"Database cleanup failed: {e}")
            raise


def main():
    """Main entry point."""
    import argparse

    parser = argparse.ArgumentParser(description="Database Cleanup Tool")
    parser.add_argument("--analyze-only", action="store_true", help="Only analyze, don't clean")
    parser.add_argument("--duplicates-only", action="store_true", help="Only remove duplicates")
    parser.add_argument("--vectorize-only", action="store_true", help="Only vectorize documents")
    parser.add_argument("--sync-only", action="store_true", help="Only sync databases")

    args = parser.parse_args()

    cleanup_tool = DatabaseCleanupTool()

    if args.analyze_only:
        duplicate_groups = cleanup_tool.analyze_duplicates()
        print(f"\nğŸ“Š Analysis complete: {len(duplicate_groups)} duplicate groups found")
    elif args.duplicates_only:
        duplicate_groups = cleanup_tool.analyze_duplicates()
        cleanup_tool.remove_duplicates(duplicate_groups)
    elif args.vectorize_only:
        cleanup_tool.vectorize_all_documents()
    elif args.sync_only:
        cleanup_tool.sync_databases()
    else:
        cleanup_tool.run_full_cleanup()


if __name__ == "__main__":
    main()
